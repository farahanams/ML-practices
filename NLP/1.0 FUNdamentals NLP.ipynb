{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals of NLP\n",
    "#### By: Farahana, Date: 13/8/2020\n",
    "\n",
    "I've never tried NLP in my life. This is my first time trying to get into this. We will try to go together from here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First model: Bag of Words\n",
    "\n",
    "In data science, it is easy to work with numerical values rather than a non-numerical values such as words. This model is the easiest model to convert sentences into numerical values/vectors. For instance;\n",
    "\n",
    "* \"I love the book\"\n",
    "* \"This is a great book\"\n",
    "* \"The fit is great\"\n",
    "* \"I love the shoes\"\n",
    "\n",
    "we can extract <1> _unique_ words from the sentences above and turn into:\n",
    "\n",
    "* \"I love the book this is a great fit shoes\"\n",
    "\n",
    "Then <2> we will create a vector for each and every sentence that has that unique word. We will implement scikit-learn [CountVectorizer](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction) from here on: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessities\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us try with the example as training set\n",
    "x_train = [\"I love the book\", \"This is a great book\", \"The fit is great\", \"I love the shoes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then let us fit it into CountVectorizer (as a dictionary) into our training set\n",
    "vectorizer = CountVectorizer()\n",
    "vectors = vectorizer.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let us check our vectors and test something with it.\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['book', 'fit', 'great', 'is', 'love', 'shoes', 'the', 'this']\n",
      "[[1 0 0 0 1 0 1 0]\n",
      " [1 0 1 1 0 0 0 1]\n",
      " [0 1 1 1 0 0 1 0]\n",
      " [0 0 0 0 1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names()) # get the unique words as features for the training set\n",
    "print(vectors.toarray()) # convert the training set into binary array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The phrase return `book`,`1`; `fit`,`0`; `great`,`0`; `is`,`0`; `love`,`1`; `shoes`,`0`; and so on as in \"I love the book\" for the first row of `vector`. We can view this in pandas dataframe below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book</th>\n",
       "      <th>fit</th>\n",
       "      <th>great</th>\n",
       "      <th>is</th>\n",
       "      <th>love</th>\n",
       "      <th>shoes</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   book  fit  great  is  love  shoes  the  this\n",
       "0     1    0      0   0     1      0    1     0\n",
       "1     1    0      1   1     0      0    0     1\n",
       "2     0    1      1   1     0      0    1     0\n",
       "3     0    0      0   0     1      1    1     0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(vectors.toarray(), columns = vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us try to make a proper dataset similar to the above training set by setting books and clothing category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Category:\n",
    "    BOOKS = \"BOOKS\"\n",
    "    CLOTHES = \"CLOTHES\"\n",
    "    \n",
    "X_train = [\"I love the book\", \"This is a great book\", \"The fit is great\", \"I love the shoes\"]\n",
    "y_train = [Category.BOOKS, Category.BOOKS, Category.CLOTHES, Category.CLOTHES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vector_train = vectorizer.fit_transform(X_train) # define the x_train as vector for the classification purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will try to use simple machine learning technique for the above dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X_vector_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can try to predict simple sentence to have the SVM classifier classes it according to its `Category`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = ['I like the book']\n",
    "X_vector_test = vectorizer.transform(X_test) # as usual, sentence is supposed to be in vector (0,1) for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BOOKS'], dtype='<U7')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X_vector_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above example is a unigram approach where each word is taken as feature. However, when we have tenses such as _\"was doing\"_ and _\"is doing\"_, and sentiment property such as _\"very good\"_ and _\"very bad\"_, we have to consider pairing the words to be more than one. Let us check a vectorizer in effect using `ngram_range` parameter.\n",
    "\n",
    ">a study by Ioannis Kanaris and others revealed that n-grams of size 3 and 4 yield good performances in the anti-spam filtering of email messages (excerpt from [Python Machine Learning Book](https://github.com/rasbt/python-machine-learning-book-3rd-edition), page 264)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['book', 'fit', 'fit is', 'great', 'great book', 'is', 'is great', 'love', 'love the', 'shoes', 'the', 'the book', 'the fit', 'the shoes', 'this', 'this is']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,2)) # With 1 and 2 words.\n",
    "vectors = vectorizer.fit_transform(X_train)\n",
    "print (vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book</th>\n",
       "      <th>fit</th>\n",
       "      <th>fit is</th>\n",
       "      <th>great</th>\n",
       "      <th>great book</th>\n",
       "      <th>is</th>\n",
       "      <th>is great</th>\n",
       "      <th>love</th>\n",
       "      <th>love the</th>\n",
       "      <th>shoes</th>\n",
       "      <th>the</th>\n",
       "      <th>the book</th>\n",
       "      <th>the fit</th>\n",
       "      <th>the shoes</th>\n",
       "      <th>this</th>\n",
       "      <th>this is</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   book  fit  fit is  great  great book  is  is great  love  love the  shoes  \\\n",
       "0     1    0       0      0           0   0         0     1         1      0   \n",
       "1     1    0       0      1           1   1         1     0         0      0   \n",
       "2     0    1       1      1           0   1         1     0         0      0   \n",
       "3     0    0       0      0           0   0         0     1         1      1   \n",
       "\n",
       "   the  the book  the fit  the shoes  this  this is  \n",
       "0    1         1        0          0     0        0  \n",
       "1    0         0        0          0     1        1  \n",
       "2    1         0        1          0     0        0  \n",
       "3    1         0        0          1     0        0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(vectors.toarray(), columns = vectorizer.get_feature_names())\n",
    "# Now, we have 16 features in the dictionary and slightly different vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, with bag of words model, anything outside the vectors data is hardly classified as it does not know what it is. Let us check with `books` instead of `book`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CLOTHES'], dtype='<U7')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = ['I like the books']\n",
    "vectorizer = CountVectorizer()\n",
    "vectors = vectorizer.fit_transform(X_train)\n",
    "X_vector_test = vectorizer.transform(X_test)\n",
    "\n",
    "clf.predict(X_vector_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier had classified the test set as `CLOTHES` category despite the only edition of the test set is a letter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Model: Word Vectors\n",
    "\n",
    "Another approach to convert to a numerical vector that has semantical meaning. Such as red and white should be mapped to colour feature. Let us check these sentences:\n",
    "\n",
    "* \"Best book I've read in years\"\n",
    "* \"great story and characters\"\n",
    "* \"no development of characters during the book\"\n",
    "\n",
    "The model is simplified into 4 steps:\n",
    "\n",
    "1. The model will look at a window of text such as 5 token long which includes \"Best book I've read in\".\n",
    "2. Then the model will look at each token in the context window and develop ideas based on its neighboring token.\n",
    "3. With training, the model will develop an association between words such as 'book' and 'read'; 'story' and 'characters'. \n",
    "4. Eventually, the model is expected to associate every related word with each other such as 'book', 'read', 'story' and 'character'. \n",
    "\n",
    "For this, we have a trained library, spacy. Follow [this](https://spacy.io/usage) to have it installed. Usually, the notebook need to restart if you haven't had it before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [nlp(text) for text in X_train] # let us use above example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check what is docs according to spacy nlp representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mType:\u001b[0m        list\n",
       "\u001b[1;31mString form:\u001b[0m [I love the book, This is a great book, The fit is great, I love the shoes]\n",
       "\u001b[1;31mLength:\u001b[0m      4\n",
       "\u001b[1;31mDocstring:\u001b[0m  \n",
       "Built-in mutable sequence.\n",
       "\n",
       "If no argument is given, the constructor creates a new empty list.\n",
       "The argument must be an iterable if specified.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "docs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 300 vectors embedding for \"I love the book\" \n"
     ]
    }
   ],
   "source": [
    "print( \"there are {} vectors embedding for \\\"{}\\\" \" .format(docs[0].vector.shape[0], docs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.08563001,  0.313255  , -0.2392405 , -0.17215225,  0.1418515 ,\n",
       "        0.1970548 ,  0.04868999, -0.12744625,  0.05947001,  2.1347    ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].vector[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use SVM the classifier to classify this word vector model for the training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vector_train_wv = [x.vector for x in docs] # make new vector training dataset using comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X_vector_train_wv, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can try with the lastest test set with `books`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BOOKS'], dtype='<U7')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_wv = [nlp(text) for text in X_test] # make a nlp list from the test set\n",
    "X_vector_test_wv = [x.vector for x in X_test_wv] # make a list of vectors\n",
    "\n",
    "clf.predict(X_vector_test_wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has recognized the plural of the book because Spacy has been trained so that its vector embedding has a semantic view of real world problem. Now, let us try with a slight different test set for classifying BOOKS and CLOTHES categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CLOTHES'], dtype='<U7')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = ['This is a beautiful handbag']\n",
    "\n",
    "X_test_wv = [nlp(text) for text in X_test] # make an nlp list from the test set\n",
    "X_vector_test_wv = [x.vector for x in X_test_wv] # make a list of vectors from it\n",
    "\n",
    "clf.predict(X_vector_test_wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVM classifier has classified the test set correctly with addition of semantic and different word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are some drawbacks for the word vector models;\n",
    "\n",
    "1. When used with larger dictionary with more classes and longer test set, spacy embedding cannot perform to the best.\n",
    "2. With two meaning words, some improvements needed to work around it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.0 Regexes\n",
    "* Not a specific python concept\n",
    "* type of regular expression\n",
    "* pattern matching of strings \n",
    "* Examples: \n",
    "    1. checking a format of phone number, whether it is valid or not;\n",
    "        * 123-123-1234\n",
    "        * +1-(123)-123-1234\n",
    "    2. checking password, has special character and uppercase.\n",
    "    \n",
    "** Regular expression [cheatsheet](https://cheatography.com/davechild/cheat-sheets/regular-expressions/) should come in handy for the coding purpose "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abcd']\n"
     ]
    }
   ],
   "source": [
    "import re # import regexes library\n",
    "\n",
    "phrase_to_match = [ 'abcd', 'xxx', 'aaa abkl ccc', 'ab cd']\n",
    "\n",
    "rexexp = re.compile (r\"^ab[^\\s]*cd$\") # check the cheatsheet\n",
    "matches = []\n",
    "for phrase in phrase_to_match:\n",
    "    if re.search(rexexp, phrase):\n",
    "        matches.append(phrase)\n",
    "        \n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to implement with books and clothes example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I liked that story.', 'I like that book.']\n"
     ]
    }
   ],
   "source": [
    "rexexp = re.compile (r\"read|story|book\") # hard-coded rule to find these words in test set\n",
    "\n",
    "test_phrases = [\"I liked that story.\", \"I like that book.\", \"This hat is nice.\"]\n",
    "matches = []\n",
    "for phrase in test_phrases:\n",
    "    if re.search(rexexp, phrase):\n",
    "        matches.append(phrase)\n",
    "        \n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the phrases 'history' and 'threaded' can be easily recognized to be in the same category for `read` and `story` as it is part of the phrases. To avoid the mistake, we can put word boundary in the hard-coded rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# let us add word boundary \\b...\\b to \n",
    "\n",
    "rexexp = re.compile (r\"\\bread\\b|\\bstory\\b|\\bbook\\b\")\n",
    "test_phrases = [\"I liked that history.\", \"the man threaded up the hill\", \"This hat is nice.\"] \n",
    "\n",
    "matches = []\n",
    "for phrase in test_phrases:\n",
    "    if re.search(rexexp, phrase):\n",
    "        matches.append(phrase)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.0 Stemming/Lemmatization\n",
    "\n",
    "* Techniques to normalize texts\n",
    "* Stemming is to reduce it to a canonical word of it such as `books` is similar to `book` and `reading` is similar to `read`\n",
    "* However, with stemming, words such as `stories` is reduced to `stori`, then, lemmatization come into workspace.\n",
    "* The `stori` now can be lemmatized to `story`.\n",
    "* There are many libraries for this, but the easiest to use is probably [nltk](https://www.nltk.org/) library (natural language tool kit).\n",
    "\n",
    "** Simple visualization and explaination can be found here: [DataCamp](https://www.datacamp.com/community/tutorials/stemming-lemmatization-python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/stemminglemmatization.jpg\" style=\"width:600px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer() # algorithm expect a single word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original phrase -reading the books- is stemmed to -read the book- using NLTK library\n"
     ]
    }
   ],
   "source": [
    "phrase = \"reading the books\"\n",
    "words = word_tokenize(phrase) # separate the phrase by words\n",
    "\n",
    "stemmed_words = []  # build an empty list for stemmed words\n",
    "for word in words:\n",
    "    stemmed_words.append(stemmer.stem(word)) # append the stemmed words in the matrix\n",
    "    \n",
    "# \" \".join(stemmed_words) # append the stemmed words\n",
    "\n",
    "print (\"The original phrase -{}- is stemmed to -{}- using NLTK library\" .format(phrase, \" \".join(stemmed_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are some drawback in this simple approach such as punctuation will be recognized as a phrase rather and words like stories become stori will single use of stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'read the stori book .'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase = \"reading the stories book.\"\n",
    "words = word_tokenize(phrase) \n",
    "\n",
    "stemmed_words = [] \n",
    "for word in words:\n",
    "    stemmed_words.append(stemmer.stem(word)) \n",
    "    \n",
    "\" \".join(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us use try _lemmatization_ with nltk library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatization = WordNetLemmatizer() # similar to stemmer, the algorithm expects a single word for lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original phrase -reading the books- is lemmatized to -reading the book- using NLTK library\n"
     ]
    }
   ],
   "source": [
    "phrase = \"reading the books\"\n",
    "words = word_tokenize(phrase) # separate the phrase for each word\n",
    "\n",
    "lemmatized_words = []  # build an empty list for lemmatized words\n",
    "for word in words:\n",
    "    lemmatized_words.append(lemmatization.lemmatize(word)) # append the lemmatized words in the matrix\n",
    "    \n",
    "# \" \".join(lemmatized_words) # append the lemmatized words\n",
    "\n",
    "print (\"The original phrase -{}- is lemmatized to -{}- using NLTK library\" .format(phrase, \" \".join(lemmatized_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to lemmatized the phrase but the default lemmatization for `WordNetLemmatizer` function is `NOUN`. The lemmatization can be changed with `POS` as in parts-of-speech. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'read the interest book'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase = \"reading the interesting books\"\n",
    "words = word_tokenize(phrase) \n",
    "\n",
    "lemmatized_words = []  \n",
    "for word in words:\n",
    "    lemmatized_words.append(lemmatization.lemmatize(word, pos='v')) # verb is used to lemmatized the phrase\n",
    "    \n",
    "\" \".join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `reading`, `interesting` and `book` words had been tagged as verb and lemmatized accordingly. Here, we can sense that it cannot identify the `interesting` as adjective and `books` as noun. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.0 Stopwords \n",
    "\n",
    "Stopwords are the most common words in the language, such as in English, this, that, he, and a. It usually get stripped out when tokenization is done. We could see this happens with the first model where `CountVectorizer` does not recognize `I`, and `a` as it has been considered as stopwords in the algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english') # the language has to be specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print (stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here example sentence demonstrating removal stopwords'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase = 'Here is an example sentence demonstrating the removal of stopwords'\n",
    "\n",
    "words = word_tokenize(phrase) \n",
    "\n",
    "stripped_words = [] \n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        stripped_words.append(word) \n",
    "    \n",
    "\" \".join(stripped_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
